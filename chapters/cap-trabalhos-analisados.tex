\chapter{Trabalhos Analisados}
\label{cap:trabalhos-analisados}

\todo[inline]{Eu sei que você gosta dessa divisão sobre a implementação, mas do ponto de vista \emph{teórico} isso não é uma divisão relevante: o trabalho ser ou não implementação leve/pesada/independente não tem nada a ver com o que o trabalho em si é. Se o autor tivesse escolhido outro estilo de implementação, não mudaria nada no conceito. Faz sentido, sim, você selecionar apenas trabalhos que tenham uma implementação (porque mostra que é possível implementar e tem até algum tipo de validação experimental), mas não gosto dessa subdivisão. Acho legal, sim, comentar essa característica dos trabalhos, em especial aqueles em que isso é muito importante, como o exokernel ou aquele que usa um módulo do kernel, mas usar para categorizar os trabalhos é bizarro (até porque ``independente'' só tem um, que é o exokernel}

Esse capítulo tem como objetivo destacar as principais pesquisas \hltodo[recentemente]{Que tal?} realizadas com
o objetivo de expandir as abstrações de processos. Para a seleção dos trabalhos
aqui apresentados, o autor levou em consideração os seguintes aspectos:
quantidade de citações dos trabalhos, \hltodo[implementação da proposta em um SO]{Acho esquisito ``em um SO''; acho que seria melhor dizer que tem que ter uma implementação funcional e não só uma proposta teórica} e
pesquisas que tenham a abstração de processos como objeto de estudo. Por uma
questão organização, esse trabalho subdivide as pesquisas usando o critério da
implementação. \hltodo[Sem entrar em detalhes,]{eu descreveria as três aqui rapidamente e estenderia um pouco a descrição na introdução de cada seção} as três categorias são:
\textit{Implementação Estrutural Leve, Implementação Estrutural Pesada e
Implementação Independente}. Por fim, essa seção visa fornecer as bases para a
discussão apresentada no Capitulo
\ref{cap:analise-sobre-abstracoes-de-processos}, tema central desse trabalho.

\section{Implementação Estrutural Leve}

Chamamos de \textit{Implementação estrutural leve} toda proposta que não altera
diretamente o núcleo do SO. Isso faz com que a proposta tenha uma chance maior
de ser incorporada em um SO de produção e, ao mesmo tempo, gera
pouco impacto geral no SO. Normalmente, implementações leves utilizam
artifícios de \emph{device drivers} (Veja \ref{sec:dd}), o que torna simples de
desenvolver e testar novas propostas.

\subsection{Dune}
\label{sec:dune}

Como mostrado na Seção \ref{sec:virtualizacao}, o conceito de virtualização já
é amplamente conhecido e adotado pela indústria; dentre os recentes avanços,
destaca-se a virtualização via hardware proporcionada pelos processadores
modernos. O mecanismo de virtualização é normalmente utilizado fornecendo
abstrações para a criação de máquinas virtuais. Com esse conceito em mente,
\citet{belay} decidiram dar outra utilidade para esses recursos de hardware:
\hltodo[fornecer]{Não seria melhor ``isolar'' processos? Ou ``fornecer recursos adicionais''?} processos ao invés de máquinas virtuais.

Os autores defendem que uma ampla variedade de aplicações pode tirar proveito
do acesso a certos recursos que são apenas disponíveis ao kernel. Dentre as
aplicações que podem comprovadamente ganhar algum desempenho ao obter acesso
direto ao hardware, destacam-se:

\begin{itemize}
  \item \textbf{Garbage Collection (GC)}\todo{Melhor ``coleta de lixo''}: O GC pode ganhar desempenho caso
        tenha como controlar diretamente o hardware de paginação, como
        \citet{pauseless} demostraram em seu trabalho;
  \item \textbf{Process Migration}\todo{Melhor ``migração de processos''}: Programas no espaço de usuário podem tirar
        proveito do acesso direto às faltas de páginas e chamadas de \hltodo[sistema]{... para fazer sei-lá-o-que com migração}.
\end{itemize}

Não obstante, fornecer o acesso direto aos recursos de hardware de forma segura
e gerenciável não é uma tarefa trivial, uma vez que requer mudanças na forma
como o espaço de usuário e o espaço do Kernel interagem. Além disso, alterações
no Kernel são consideradas invasivas e perigosas, uma vez que uma modificação
feita de forma errada pode comprometer toda a estabilidade do sistema. Outra
alternativa para dar acesso de hardware para aplicações é por meio \hltodo[de imagens
de máquina virtual,]{Melhor algo tipo ``de execução em uma máquina virtual''} assim uma aplicação consegue acesso seguro aos recursos de
hardware através da interface de virtualização. Uma desvantagem desta abordagem
é que ela tem pouca integração com o SO da máquina \emph{host}, o que impede o
processo de obter ganhos de desempenho.

\citet{belay} oferecem uma alternativa que consiste em utilizar a abstração de
virtualização disponível nos processadores modernos de forma a fornecer um novo
tipo de processo que executa no chamado \emph{Dune Mode} (modo Dune). Tal modo
é um estado irreversível no qual o processo indica explicitamente que quer
entrar; nesse modo, o processo passa a ter acesso direto, por meio dos recursos
de virtualização, a modos de operação com maior privilégio, registradores de
memória virtual, tabelas de páginas, interrupções, exceções e vetores de
chamada de sistema. Para auxiliar a aplicação no espaço de usuário a interagir
com tais recursos, os autores criaram uma biblioteca chamada \emph{libDune} que
dá apoio à utilização das novas funcionalidades.

Dentre as vantagens que o essa pesquisa promete, destacam-se o fato de que no
modo Dune o processo comporta-se como um processo normal, com a diferença de que
ele utiliza a instrução \hltodo[VMCALL para fazer chamadas de sistema]{E por que isso é uma vantagem?}. Além disso,
devido ao fato de que o Dune não tem por objetivo fornecer máquinas virtuais,
ele consegue ser mais simples e rápido. As aplicações que se
adequam ao uso desse modelo podem obter \hltodo[melhorias de desempenho e isolamento.]{Acho que precisa explicar um pouco mais e melhor qual a vantagem ou dizer tipo ``como veremos adiante'' ou algo assim}

Para implementar o Dune, os pesquisadores utilizaram processadores Intel com
suporte para a tecnologia VT-x (veja a Seção \ref{sec:vtx}) e parte do código do KVM já
disponível no Kernel Linux. O Dune apresenta três tipos de instruções
privilegiadas para os processos:

\begin{itemize}
	\item \textbf{Exceções}:  As instruções \hltodo[LIDT, LTR, IRET, STI e CLI]{Não rola explicar o que elas fazem? Pode ser uma explicação de meia dúzia de palavras sobre cada uma ou uma explicação geral sobre o conjunto delas. Isso vale para os próximos itens também} são
        expostas diretamente para os processos. Note que emulação, depuração e
        rastreamento de desempenho são atividades que podem tirar proveito
        desta característica, uma vez que o Dune pode reduzir consideravelmente
        a sobrecarga em entregar exceções. Isso se deve ao fato que o Dune
        consegue entregar exceções diretamente para o \hltodo[hardware]{Para o software, não?} de forma segura
        (encapsulado pelo hardware de virtualização);
  \item \textbf{Memória Virtual}: As instruções \texttt{MOV CRn, INVLPG} e
        \texttt{INVPCID} são oferecidas para o processo Dune. O acesso
        flexível à memória virtual pode ser benéfico para aplicações que fazem
        \emph{checkpointing}, GC, compressão de dados nas páginas e utilizam memória
        compartilhada distribuída. O Dune traz melhorias para o acesso à
        memória virtual expondo as entradas da tabela de páginas para as
        aplicações. Isso permite que elas controlem a tradução de endereços,
        as permissões de acesso, bits globais e que elas modifiquem/acessem as páginas com
        simples referências. Por fim, o Dune também dá a habilidade de
        controlar as invalidações da TLB manualmente, o que pode ser vantajoso
        para alguns tipos de aplicações;
  \item \textbf{Modos privilegiados}: As instruções \texttt{LGDT} e
        \texttt{LLDT} permitem que o Dune exponha os modos privilegiados de
        forma segura. O Dune expõe esse modos de forma eficiente e segura por
        que o VMX non-root mantém o seu próprio conjunto de anéis privilegiados
        (Veja \ref{sec:vtx}).
\end{itemize}

A Figura \ref{fig:dune_architecture} ilustra a arquitetura do Dune. O Dune
estende o Kernel como um módulo que habilita o VT-x, colocando-o no
modo VMX root. Assim, processos usando Dune têm acesso seguro e garantido para
hardware privilegiado utilizando o modo VMX non-root. O módulo Dune intercepta
os \hltodo[VM exits]{Seria bom explicar wtf}, o único motivo para o processo acessar o kernel e realizar
qualquer ação. Repare que o Dune é aplicado de forma seletiva aos processos que
de fato precisam dele, o que significa que um processo que não usa o Dune não é
afetado. A transição de um processo normal para o modo Dune ocorre por meio de
uma chamada de \texttt{ioctl} no dispositivo \texttt{/dev/dune} e, ao
entrar nesse modo, o processo não pode retornar ao modo anterior. Sempre que um
processo Dune faz um \texttt{fork()}, o processo filho não inicia em modo Dune,
contudo pode reentrar caso queira.

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.6\textwidth]{dune_architecture} 
	\caption[Arquitetura do Dune]{Arquitetura do Dune~\citep{belay}}
  \label{fig:dune_architecture}
\end{figure}

Na Seção \ref{sec:virtualizacao}, apresentamos as principais propriedades dos VMMs.
Com o objetivo de deixar claras as diferenças entre o VMM e as características
de hardware expostas pelo Dune, destacamos as principais diferenças:

\begin{itemize}
  \item No Dune, o mecanismo de \textit{hypercall} invoca
        \textit{system calls} comuns do Linux;
  \item Apenas características de hardware que podem ser acessadas diretamente,
        sem a intervenção do VMM, são disponíveis para a aplicação. Nos casos em
        que não é possível executar uma dada operação, Dune devolve o controle
        para o SO;
  \item Podemos limitar as diferenças entre \textit{guest} e \textit{host}, uma
        vez que processos usando Dune têm uma interface próxima da de hardware;\todo{não entendi este item}
  \item No Dune, é preciso configurar o \hltodo[EPT]{dafuq?} para refletir o endereço do espaço
        de usuário.
\end{itemize}

\subsection{Shreds}

Existe uma categoria de ataques chamada ``abusos intra-processos de conteúdo
de memória'', na qual o invasor busca acessar determinado tipo de conteúdo
sensível na memória (e.g., dados secretos) ou uma região de código crítica
(e.g., funções privilegiadas) \citep{shreds}. O roubo desse tipo de dados
acontece por meio da introdução de algum código injetado ou por meio de alguma
biblioteca maliciosa que permite acessar alguma região da memória não protegida
da vítima. Note que bibliotecas maliciosas são especialmente perigosas, uma vez
que elas podem executar funções privilegiadas que são de fato parte do
processo como, por exemplo, a função \texttt{dlopen}. Por fim, se o atacante
conseguir o acesso a memória, ele pode ler todos os dados buscando por outras
informações.

\citet{shreds} defendem que o problema de segurança referente aos ataques
intra-processos decorrem da falta de controle fino e eficiente do conteúdo
sensível da memória; além disso, eles argumentam que seria benéfico se tais
funcionalidades fossem disponibilizadas para os programadores.  Para resolver tal
problema, os autores apresentam uma nova abstração de processos que cria uma
nova unidade de execução chamada \hltodo[\emph{shreds}]{Não seria só ``shred'' aqui, no singular? Tem várias dessas inconsistências nesta seção, inclusive ``a shred'', ``o shred'' etc.}, que tem associado a si um
fragmento de memória protegida denominado \emph{shred-private pool
(s-pool)}. O tamanho do segmento de memória criado é variável,
dependendo da necessidade da aplicação. Além disso ele se comporta de forma a
existir apenas durante o momento em que é necessário para um pedaço da
aplicação. A implementação de \emph{shreds} é baseada em três pilares: uma API
para o usuário conseguir utilizar facilmente as funcionalidade oferecidas, um
\hltodo[\emph{toolchain} de compilação]{que tal ``um conjunto de ferramentas de compilação'' ou ``um processo de compilação'' ou algo assim?} que verifica a correta utilização dos
\emph{shreds} (\emph{S-compiler}) e um módulo Linux que faz o gerenciamento da
memória (\emph{S-driver}).

\emph{Shreds} foi projetado buscando três propriedades que visam garantir
segurança:

\begin{itemize}
  \item \textbf{Acesso exclusivo para s-pool:} Um \emph{s-pool} só é acessível
        para o \hltodo[\emph{shreds}]{De novo, acho que seria singular} associado a ele;
  \item \hltodo[\textbf{Evitar vazamentos na entrada e saída:}]{Você disse ``três propriedades''; a primeira é um substantivo (``acesso exclusivo''; a terceira é um substantivo (``execuções''; é esquisito que a segunda não seja um substantivo. Melhor seria tipo ``controle de vazamentos na entrada e saída'' ou algo assim} Dados carregados em um
        \emph{s-pool} não podem ser copiados ou exportado sem ter\todo{meio esquisita essa frase; prefiro algo como ``sem que seja executada uma operação de limpeza'' ou algo assim} uma operação
        de limpeza antes;
  \item \textbf{Execuções não desviáveis:} O fluxo de execução não pode ser
        alterado para fora do \emph{shreds}.
\end{itemize}

Para melhor entender o comportamento de \emph{shreds}, veja a Figura
\ref{fig:shreds}, ilustrando uma aplicação que lida com uma senha (dado
sensível) e o código que manipula tal dado. A aplicação possui uma thread que
comporta-se da forma padrão, contudo, ao ter que lidar com um dado sensível, ela
faz uma chamada para uma função da API fornecida por \emph{shreds},
\texttt{shred\_enter()}. Ao realizar a chamada para essa função, o fluxo de
execução entra em um modo seguro no qual \emph{shreds} gerencia o acesso à
memória e garante que os dados são isolados \hltodo[no sistema]{que tal ``... do restante da aplicação''?}. Por fim, após realizar
a operação na região protegida, a aplicação do usuário chama
\texttt{shred\_exit()}. Para utilizar a proposta dos autores, é preciso alterar
as aplicações alvo.

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.6\textwidth]{shreds} 
  \caption{Funcionamento geral de shreds}
  \label{fig:shreds}
\end{figure}

A API fornecida por \emph{shreds} consiste em apenas quatro funções:

\begin{itemize}
  \item \texttt{shred\_enter()}: Inicia a execução na thread atual, faz uma
        troca para execução em uma stack alocada para um \textit{s-pool} e
        espera um descritor para um pool;\todo{Está meio jogado isso aqui; que pool? Que descritor?}
  \item \texttt{shred\_exit()}: Suspende o shred chamado, revoga as permissões
        de acesso da thread atual sobre o \textit{s-pool} e recupera a stack de execução
        original;
  \item \texttt{spool\_alloc()}: Aloca memória para o s-pool associado à
        thread;
  \item \texttt{spool\_free()}: Apaga a memória alocada.
\end{itemize}

Para garantir que as regras de utilização da API sejam seguidas, o
\emph{S-compiler} realiza verificações e instrumentações para evitar falhas que
abram brechas de segurança.

Para este trabalho, a parte mais importante de \emph{shreds} é o \emph{S-driver},
que consiste em um módulo do Linux responsável por gerir as \emph{shreds} e
\emph{s-pools}. A implementação dessa técnica é baseada em domínios de memória
do ARM (Seção \ref{sec:outros_mecanismos_memoria}). Contudo, dado que essa
tecnologia não foi projeta para os propósitos sugeridos pelo \emph{shreds}, ela
apresenta duas limitações:

\begin{itemize}
  \item O ARM fornece apenas 16 domínios, o que impede que se crie um domínio
        para cada \emph{s-pool}, uma vez que seu número pode crescer de forma
        indeterminada;
  \item O controle de acesso à memória é extremamente limitado.
\end{itemize}

A solução adotada pelos autores foi implementar um mecanismo que multiplexa os
domínios e introduz identidades dentro da lógica de acesso. Para permitir que
uma aplicação tenha várias \emph{shreds}, o \emph{S-driver} utiliza os domínios
de forma temporária, rotacionando as identidades de segurança para o
\emph{s-pool}. Toda vez que uma \emph{shreds} inicia ou retoma a sua execução
em uma $CPU_i$ o \emph{S-driver} atribui um \emph{s-pool} associado a um
domínio $Dom_i$. Isso faz com que uma \emph{shred} consiga acessar o seu
\emph{s-pool} enquanto outras threads concorrentes não. Quando o
\emph{shred} finaliza ou tem a sua execução parada em favor de outro
processo, o \emph{S-driver} muda o domínio, prevenindo que outro processo
consiga acessar a região de memória. Note que o driver permite ou rejeita o
acesso a um \emph{s-pool} baseado na CPU; isso faz com que, mesmo que um código
malicioso consiga executar em paralelo com o \emph{shred}, ele não consiga ter
acesso ao \emph{s-pool} sem que seja disparada uma falha de domínio.

Uma das funcionalidades fornecidas pelo \emph{shread} é a habilidade de
permitir fazer cópias de dados para regiões de memória não protegidas de forma segura
(e.g., atribuir dados de um \emph{s-pool} para uma variável local). Note que,
para realizar tal procedimento, é preciso acessar uma stack regular que pode ter
sido atacada e conter um código mal-intencionado, o que violaria toda proteção
fornecida pelo \emph{s-compiler} e \emph{S-driver}. Para prevenir que dados
sejam vazados via stack, o \emph{S-driver} cria uma stack segura para cada
\emph{shred} alocada partindo do \emph{s-pool} associado a si.

\section{Implementação Estrutural Pesada}

As \textit{Implementações Estruturais Pesadas} são aquelas que alteram
diretamente o núcleo do SO. Essa abordagem traz vantagens e desvantagens que
devem ser consideradas antes de serem adotadas. Por um lado, alterações diretas
no núcleo são mais flexíveis em termos de possibilidade de acesso aos recursos
disponíveis do SO. Por outro lado, alterações no núcleo são arriscadas, uma vez
que podem afetar toda a estabilidade do sistema, e são extremamente complexas de
serem feitas, uma vez que demandam profundo conhecimento do SO em questão.

\subsection{Wedge}

Existe um princípio chamado de \emph{``Princípio do Menor Privilégio''} que diz:

\begin{quote}
Divida o código em compartimentos na qual cada um vai executar com o menor
privilégio para completar a sua tarefa. Essa abordagem previne contra ataques
e também previne bugs que levam ao vazamento de dados.
\citep{protectionprinciple}\todo{Este texto está um pouco estranho. Suponho que seja uma tradução sua; qual o texto original?}
\end{quote}

Apesar desse princípio ter sido apresentado em 1975 e ser amplamente aceito,
nota-se que várias aplicações que tem acesso à Internet (um ponto de acesso
para o mundo) não buscam atender a esse fundamento e acabam abrindo
potenciais pontes de vazamento de dados ou falhas de segurança. Alguns
pesquisadores defendem que esse problema surge do fato de que os SO atuais
garantem privilégios por padrão. Isso significa que impor limites requer mais
esforço por parte do programador e também é propenso a erros.

A chamada de sistema \texttt{fork()} ilustra a característica de garantir
privilégios por padrão. Quando o processo pai cria um filho, este inicia como uma
cópia praticamente exata do pai. Se o programador desejar restringir o nível de
acesso do processo filho, é preciso realizar uma série de operações manuais que
não garantem que nenhum aspecto tenha sido ignorado.

\citet{wedge} buscaram resolver tal problema mudando o princípio atual, de
garantir privilégio por padrão, para negar privilégios por padrão. Nessa
abordagem, o programador deve indicar explicitamente que deseja garantir
permissões adicionais. Os autores alteraram o GNU/Linux para que ele
suportasse a remoção de permissões por padrão e chamaram o modelo proposto por
eles de \emph{Wedge}. \emph{Wedge} fornece um conjunto de primitivas de
programação para permitir criar componentes com semântica de negação de permissões
por padrão, tornando o acesso mais restrito. Para isso, \emph{Wedge} oferece
um esquema simples e flexível de marcação (\emph{tagging}) das permissões de
acesso à memória, permitindo que o programador aloque objetos na memória de
acordo com essa informação.

\emph{Wedge} permite que o programador crie um número arbitrário de
componentes, cada um sem privilégios por padrão, mas existem mecanismos que
permitem o controle fino. Neste sentido, \emph{Wedge} busca se assemelhar às
primitivas atuais para facilitar a sua adoção e para que seja simples a
introdução delas em aplicações legadas. A proposta é constituída de três
primitivas: \emph{Sthread}, \emph{Tagged Memory} e \emph{callgates}. O
acesso a todas essas primitivas é feito via chamadas de sistema.

Uma \emph{Sthread} pode ser comparada com uma thread fornecida pela biblioteca
\textit{pthread}, definindo um compartimento dentro de uma
aplicação. O programador pode então atribuir permissão de acesso para a memória
e outros recursos por meio dessas \emph{sthreads}. Elas consistem de uma
thread de controle e políticas de segurança associadas, que especificam:

\begin{itemize}
	\item As marcações de memória que a \emph{sthread} pode acessar e a permissão
				sobre cada cada uma (leitura, leitura-escrita e \textit{copy-on-write});
	\item Os descritores de arquivo que a \emph{sthread} pode acessar e as
				permissões sobre cada (leitura, escrita, leitura-escrita);
	\item Os \emph{callgates} que a \emph{sthread} pode chamar;
	\item O \emph{Unix user id}, o \hltodo[\emph{root directory}]{Diretório raiz, pô :-p} e o \hltodo[\emph{SELinux}]{Acho que seria ``contexto SELinux'' ou algo assim, não?}.
\end{itemize}

Uma nova \emph{sthread} não mantém as permissões de acesso por padrão, a não
ser \hltodo[pelo \emph{snapshot} intocado do \emph{copy-on-write} acessado]{dafuq?}. Note que o
novo componente não tem permissão para acessar qualquer outra memória ou mesmo
descritor de arquivo do pai, contudo o pai pode anexar permissões de acesso no
filho durante a sua criação. Uma \emph{sthread} só pode criar um filho com
permissão igual ou menor do que a sua. Para a implementação, os autores criaram
\emph{sthreads} como uma variante dos processos do GNU/Linux, só que, ao invés
de herdar as informações do pai, ela apenas herda as regiões de memória e tabela
de descritores asseguradas pela política de segurança.

\todo[inline]{No próximo parágrafo, não entendi o funcionamento da criação das tags, alocação, segmento etc.}

Quando alguma alocação de memória ocorre, o programador deve marcar a memória
com uma única \emph{tag}; as permissões de acesso a esta memória são garantidas
em termos delas. Por exemplo, pode-se definir leitura e escrita para uma
memória com a \emph{tag} ``t''. O programador expressa os privilégios de memória
em termos das \emph{tags} que são atribuídas para a memória em tempo de
alocação. Vale acrescentar que uma \emph{tag} nunca implica em privilégios de
outra \emph{tag}. A operação de criação de \emph{tags} começa pela criação delas
por parte do programador. Essa operação aloca um segmento de memória e armazena
um mapa da \emph{tag} para o segmento. Em seguida o programador chama a função
para alocar a memória marcada (\texttt{smalloc()}) especificando a \emph{tag} o
tamanho solicitado e assim aloca um \emph{buffer} do segmento com a \emph{tag}.

Um \emph{callgate} é uma porção de código que executa em um nível de privilégio
(normalmente) diferente de quem o chamou. Como um \emph{sthread} sempre roda
com o menor privilégio possível, sempre que for preciso executar algo em um
nível de permissão maior, será preciso chamar um \emph{callgate} para executar
a operação. Para criar e usar um \emph{callgate}, é necessário indicar um ponto
de entrada, um conjunto de permissões e argumentos confiáveis fornecido pelo
responsável em criar o \emph{callgate}. \emph{Wedge} garante que os
argumentos passados e criados não podem ser adulterados pelo processo que o
utilizará. Além disso o \emph{callgate} herda o \hltodo[sistema de arquivos]{o diretório raiz, não? Ou o diretório atual?} e o id do
usuário que o criou. As permissões do \emph{callgate} precisam ser um
subconjunto de quem o criou. Depois que um \emph{callgate} foi criado, a
\emph{sthread} privilegiada pode criar um filho com menos privilégios mas com
acesso ao \emph{callgate} criado. Quando o filho tenta acessar o
\emph{callgate}, uma nova sthread é criada e a execução é levada para o ponto de
entrada que foi registrado.

Para a implementação dos \emph{callgates}, a função \texttt{sc\_cgate\_add()}
adiciona permissões para o ponto de entrada \texttt{cgate} com as permissões e
os argumentos confiáveis, e todos esses dados são salvos no Kernel para evitar
acessos indevidos. Um \emph{callgate} é invocado por meio da função
\texttt{cgate()}, que faz com que o Kernel verifique as permissões. De forma
geral, a API de \emph{Wedge} é definida pelas seguintes funções:

\begin{itemize}
  \item \texttt{sthread\_create(sthread\_t, sc\_t, cb\_t, void *arg)}
  \item \texttt{sthread\_join(sthread\_t, void **ret)}:
  \item \texttt{tag\_t tag\_new()}
  \item \texttt{tag\_delete(tag\_t)}
  \item \texttt{smalloc(int sz, tag\_t tag)}
  \item \texttt{sfree(void *x)}
  \item \texttt{smalloc\_on(tag\_t tag}
  \item \texttt{smalloc\_off()}
  \item \texttt{sc\_mem\_add(sc\_t *sc, tag\_t t, unsigned long prot)}:
  \item \texttt{sc\_fd\_add(sc\_t, char *sid)}
  \item \texttt{sc\_cgate\_add(sc\_t sc, cg\_t cgate, sc\_t *cgsc, void *arg)}
  \item \texttt{cgate(cg\_t cb, sc\_t *perms, void *args)}
\end{itemize}


\subsection{Resource Container}

\citet{resourcecontainers} argumentam que as abstrações de SO têm recebido
grandes contribuições para melhorar questões de desempenho, mas que
pouca atenção tem sido dada para o gerenciamento de recursos. Como exemplo, os
autores mostram como a gerência de recursos é um elemento crítico para conter
ataques como o \emph{Denial-of-Service Attack}, que basicamente consiste em fazer
com que todos os recursos da máquina sejam consumidos. Eles também
assumem que a raiz dos problemas de gestão de
recursos encontra-se no modelo atual dos SOs de propósito geral, em
que primitivas de escalonamento e de gerência de recursos estendem-se por todo
o Kernel. Esse acoplamento torna complicado o processo de permitir que uma
aplicação controle os seus próprios recursos.

\begin{figure}[!h]
  \centering
  \includegraphics[width=\textwidth]{resource_constainer_scenarios} 
  \caption{Cenários das aplicações}
  \label{fig:resource_constainer_scenarios}
\end{figure}

Para ilustrar as questões referentes à gestão de recursos nos SOs, a Figura
\ref{fig:resource_constainer_scenarios} mostra 4 cenários diferentes. O
primeiro cenário consiste em uma aplicação rodando em \textit{user space} que
não necessita de qualquer recurso diretamente controlado pelo Kernel e,
portanto, cujo consumo de recursos não se estende para o Kernel. O segundo
cenário representa uma aplicação que precisa utilizar recursos do Kernel para
terminar alguma de suas tarefas (e.g., um software que usa recursos de rede).
Note que parte da aplicação consome recursos dentro do \textit{user space};
contudo, a outra parte da aplicação se expande para o Kernel. O terceiro cenário
mostra múltiplos processos cooperando para concluir uma tarefa sem precisar
do Kernel. Por fim, temos uma aplicação com um único processo que dispõe de
múltiplas threads que, além de utilizar recursos no \textit{user space}, também
demanda recursos oriundos do Kernel. Repare como sistemas tradicionais têm
pouco ou nenhum controle sobre os recursos consumidos;esses casos
podem levar o escalonador a fazer contagens imprecisas levando-o a
comportar-se de forma inadequada.

Inspirados pela falta de uma separação clara do gerenciamento de recursos por
parte dos SOs, \citet{resourcecontainers} sugeriram a criação de uma nova
abstração chamada \emph{Resource Container (RC)}. Este é uma entidade que
logicamente contém todos os recursos utilizados pela aplicação para completar
uma determinada atividade. Seus atributos são usados para fornecer parâmetros
de escalonamento, limites de recursos, e \emph{QoS} para rede. A ideia é que o
Kernel cuidadosamente manipule esses recursos, tal qual CPU e memória, consumidos
por um RC. O escalonador pode acessar as informações provenientes dos RCs e
utilizá-las para melhorar o processo de escalonamento.

A adição do RC cria uma clara distinção entre o domínio da proteção e o de
recursos, além de fornecer um controle fino sobre os recursos. O RC fornece um
mecanismo para ligar os recursos a uma thread, de forma
dinâmica e sob controle da aplicação (isso também é chamado de \emph{resource
binding}). Note que isso permite que múltiplas threads sejam associadas a
inúmeros RCs simultaneamente, mas por padrão, uma thread começa com um RC
herdado do processo pai. A aplicação poder religar-se a qualquer outro RC
caso exista a necessidade.

Uma das implicações diretas do RC é que uma aplicação pode associar informações a
uma atividade ao invés de a uma thread ou processo, permitindo que
o \emph{scheduler} forneça informações diretas para a atividade.

% TODO: Voltar no artigo e elaborar mais essa questão dos atributos
A alocação de atributos está diretamente associada com um modelo de
escalonamento relacionado com cada RC em um sistema. Uma thread é normalmente
escalonada de acordo com os atributos do RC ao qual é limitada. Contudo, se uma
thread é multiplexada entre vários containers, o escalonamento torna-se custoso
para cada mudança de \textit{binding}. Na prática, a thread deve ser escalonada com base
na combinação entre alocação de recursos e uso de RC. Para isso, o modelo
define um \emph{scheduler binding} entre cada thread e um conjunto de containers
sob o atual sendo multiplexado. 

É interessante observar que os containers formam hierarquias, ou seja, o uso de
um recurso usado por um container filho é restringido pelos parâmetros do
container pai. Por exemplo, se o container pai tem 70\% dos recursos
garantidos, então os container filhos não podem extrapolar esse limite.

Para se utilizar RCs, os autores sugerem uma API com as
seguintes características:

\begin{itemize}
	\item \textbf{Criar um Novo Container:} Por padrão, um container é criado
				como parte do \texttt{fork()} e é visível para a aplicação na forma
				de um descritor de arquivos;
	\item \textbf{Ajuste do Container \hltodo[API]{Acho que aqui era ``Pai''}:} Um processo pode mudar quem é o seu
				container pai;
	\item \textbf{Liberar o Container:} Um processo pode liberar a sua referência
				para um container usando \texttt{close()};
	\item \textbf{Compartilhar Container entre Processos:} Quando um processo
				recebe uma referência para um container, ele pode usar o recurso como
				um \emph{resource context}. Isto permite mover uma aplicação ou
				computação entre vários domínios de proteção;
	\item \textbf{Atributos do Container:} Uma aplicação pode ler e ajustar
				atributos de um container;
	\item \textbf{Uso de Informações dos Containers}
	\item \textbf{Binding threads para um container:} Uma thread pode fazer o
		\emph{binding} para uma \emph{thread} a qualquer momento\todo{Oi?!};
	\item \textbf{Restaurar o Scheduler Binding}
	\item \textbf{Binding de Socket ou Arquivo para um Container:} Um processo
				pode fazer o \emph{bind} de um socket ou arquivo para um container,
				assim o consumo subsequente dos recursos do kernel ficam no container.
\end{itemize}

\subsection{Nooks}
% deixar claro que tudo isso é no nível do kernel
\citet{nooks} conduziram uma pesquisa motivada pela busca por
adicionar mais confiabilidade e resiliência aos SOs de produção. O principal
problema que os autores buscam resolver são as constantes falhas geradas pela
expansão dos SOs por meio dos programas que controlam um dispositivo, também
conhecidos como \emph{modules} ou \emph{device drivers} (Seção \ref{sec:dd}). O
artigo apresenta diversas evidências relacionadas às falhas geradas
pelas extensões feitas nos SOs e sugere uma nova abordagem chamada
\emph{Nooks}.

\emph{Nooks} é uma nova camada que se interpõe entre o Kernel e suas extensões.
Essa camada comporta-se como um subsistema responsável por tratar as operações
passadas via kernel para os módulos e vice-versa; o controle feito sobre cada
parte representa uma camada a mais de verificação que adiciona mais segurança e
confiabilidade para o SO. Para fornecer a separação e controle dentro do SO,
\emph{Nooks} apresenta o conceito de \emph{lightweight kernel protection
domain} (LKPD). Esta abstração permite isolar partes da memória no nível do
Kernel e atribuir diferentes permissões de leitura e escrita para a região.
Note que todo o controle entre as regiões de memória e a comunicação entre o
kernel com as extensões ocorre no domínio do kernel, não tendo relação direta
com o \emph{user space}.

O projeto de \emph{Nooks} foi totalmente guiado por dois princípios
fundamentais:

\begin{enumerate}
	\item \emph{Nooks} deve ser resistente a falhas e não tolerante a falhas;
	\item \emph{Nooks} deve ser projetado para evitar erros e não para evitar
				abusos.
\end{enumerate}

Esses dois princípios são importantes para deixar clara a área de atuação desta
técnica e para guiar os três objetivos principais que o \emph{Nooks} busca
atender:

\begin{enumerate}
	\item \textbf{Isolamento:} O \emph{Nooks} precisa ser capaz de isolar as
				extensões presentes no Kernel de forma a evitar falhas. Consequentemente,
				ele precisa ser capaz de detectar falhas antes que outras partes do
				SO sejam afetadas;
	\item \textbf{Recuperação:} A arquitetura precisa dar suporte para a
				recuperação da aplicação em caso de falhas;
	\item \textbf{Compatibilidade com versões anteriores:} \emph{Nooks} deve ser
				compatível com as extensões já existentes e usadas.
\end{enumerate}

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.8\textwidth]{nooks_nim}
	\caption[Visão geral da arquitetura do Nooks]{Visão geral da arquitetura do Nooks \citep{nooks}}
  \label{fig:nooks_nim}
\end{figure}

A Figura \ref{fig:nooks_nim} ilustra de forma geral a arquitetura de
\emph{Nooks}. Note que a comunicação entre o Kernel e as extensões (e vice-versa)
precisa passar pelo \emph{Nooks}. Esse intermédio é conduzido pelo \emph{Nooks
Isolation Manager (NIM)}, que é o responsável por implementar os objetivos do
projeto. \emph{Nooks} precisa estender-se em parte para o Kernel e em parte
para as extensões. No Kernel, é preciso alterar aquelas funções que são
disponibilizadas para as extensões ou que interagem com os módulos. Felizmente,
esse tipo de função pode ser detectado por algum padrão que, por sua vez, pode
ser abstraído para um \emph{script}. Além disso, esse tipo de modificação precisa ser
feito uma única vez. Do ponto de vista dos módulos, no geral, são poucos os
casos que precisam ser alterados diretamente, uma vez que eles fazem uso dos
recursos fornecidos pelo Kernel. \hltodo[Normalmente, módulos que exportam estruturas de
dados próprias e que são exportáveis para o Kernel, consequentemente precisam
ser alteradas.]{Não entendi esta frase}

O \emph{Nooks} fornece isolamento por meio do LKPD: toda extensão
adicionada ao SO executa dentro do seu próprio LKPD que, por sua vez, é chamado
de \emph{contexto de execução}. Os domínios utilizam o mesmo nível de proteção
fornecido pelo processador para o Kernel, com a diferença de que o acesso de
escrita para certas porções é limitada e gerenciada pelo \emph{NIM}. Neste
sentido, podemos dividir o esquema de isolamento de \emph{Nooks} em duas partes:
o gerenciador de memória e o \emph{Extension Procedure Call} (XPC).

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.8\textwidth]{nooks_mem}
  \caption[Acesso à memória do Nooks]{Acesso à memória do Nooks \citep{nooks}}
  \label{fig:nooks_mem}
\end{figure}

O gerenciador de memória de \emph{Nooks} é responsável por alocar, desalocar e
manipular os LKPDs. A Figura \ref{fig:nooks_mem} ilustra o \textit{kernel
address space}: note que o Kernel tem permissão de escrita e leitura sobre toda
a memória, mas os LKPDs mostrados só têm acesso de leitura e escrita a si mesmos.
A leitura de outras regiões de memória de dentro de um LKPD depende do controle
de acesso feito pelo NIM mas, no geral, só é permitida a leitura de outras
regiões da memória.

O outro mecanismo de isolamento fornecido pelo \emph{Nooks} é o XPC, que atua
como um serviço de transferência de controle criado para isolar as operações
feitas dentro do Kernel. A transferência de controle pode acontecer em duas
vias: do Kernel para a extensão (\texttt{nooks\_driver\_call}) e da extensão
para o Kernel (\texttt{nooks\_kernel\_call}). Essas funções esperam
três argumentos: um ponteiro para a função que será executada, uma lista de
argumentos e o domínio em que será executada. Toda vez que a rotina de
transferência é chamada, o contexto da stack que realizou a chamada é salvo e o
\emph{Page Table} para o novo domínio alvo carregado.

Um dos objetivos do \emph{Nooks} é o de ser usado em SOs de produção. Para isso,
ele tem de se integrar com o SO de forma a causar o menor impacto possível
nas extensões. Por isso, \emph{Nooks} isola as extensões com XPC e também
implementa um mecanismo de rastreamento de estruturas de dados chamado
\emph{object-tracking} (parte do \emph{NIM}). O \emph{object-tracking} tem três
recursos básicos: (1) manter uma lista de estruturas de dados do Kernel que são
manipuladas por uma extensão, (2) controlar todas as modificações para essas
extensões e (3) fornecer informações de objetos para limpeza de dados quando a
extensão falhar. O rastreamento dos objetos começa com o armazenamento de todos
os objetos em uso por uma extensão e em seguida é feita uma associação entre o
Kernel e as versões das extensões.

Por fim, \emph{Nooks} possui um mecanismo de recuperação que busca detectar
falhas de software quando uma extensão é chamada de forma inapropriada ou se
uma extensão está consumindo muitos recursos. A estratégia de recuperação é
subdividida em duas partes: \emph{recovery management} e \emph{user-mode
agent}. O primeiro é responsável por desalocar recursos e, se necessário,
realocá-los. O segundo é quem coordena a ação de recuperação e
é definido no espaço de usuário. Note que a flexibilidade do \emph{agent-mode}
permite várias possíveis formas de recuperação, sendo a mais direta forçar a
remoção da extensão, seguida da reinserção e em seguida prosseguindo com a
execução da aplicação.

\emph{Nooks} utilizou uma base de dados de inserção de falhas em módulos para
validar a sua eficácia, conseguindo tratar 99\% dos casos. Além disso, os
autores também validaram o trabalho da perspectiva do desempenho e notaram que,
na maioria dos casos, os gastos com a estrutura são compensados pelas vantagens.

\subsection{Mondrian Memory Protection e Mondrix}

\citet{mmp} buscaram explorar técnicas para realizar controle fino sobre da
memória, com o objetivo de adicionar mais confiabilidade e segurança para os
SOs. Os autores discutem os problemas associados com a decisão usual de se promover
isolamento por meio da separação do espaço de endereçamento dos processos, que
por sua vez faz com que todas as \emph{threads} de um processo compartilhem um
mesmo domínio de proteção. Além disso, o modelo de paginação comumente adotado impõe um
tamanho fixo de página. Logo, o menor compartilhamento possível é o de uma página, que
tem o mesmo nível de permissão para todos os elementos contidos nela. Os autores
argumentam que, apesar das vantagem oferecidas pelo modelo atual, muitos
aspectos de segurança e estabilidade do sistema são comprometidos. Dois
exemplos são apontados pelos autores; o primeiro ilustra o caso do servidor
Apache, que pode carregar vários \textit{plugins} externos, contudo, se algum
código malicioso ou defeituoso for carregado ele comprometerá toda a
estabilidade do servidor (em alguns casos quebrando a aplicação). Outro exemplo
refere-se ao acesso de ponteiros em outros domínios da memória (por exemplo,
acesso do espaço de usuário para o do Kernel), que atualmente não são permitidos,
sendo necessário realizar operações de cópias.

% TODO: Figura Y

Os autores defendem que mecanismos de controle fino do acesso à memória podem
trazer grandes benefícios em termos de segurança, confiabilidade e, em certos
casos, desempenho. Nesse sentido, \citet{mmp} propuseram uma abordagem
que permite o controle de acesso à memória no nível do tamanho das palavras de
dado. Para isso é apresentado um novo tipo de abordagem baseada em hardware e
software chamado \emph{Mondrian Memory Protection (MMP)}~\citep{mmp}. Além
disso, uma implementação prática chamada \emph{Mondrix}~\cite{mondrix} foi
feita no GNU/Linux. \hltodo[A Figura Y]{Precisa fazer ou tirar isto} resume a representação de memória que o MMP
busca atingir. Repare que não existem porções de memória específica; a
memória é dividida em diversos compartimentos chamados \emph{Protection
Domains (PD)}. Um PD é definido como um contexto que determina a permissão para
executar um código. Cada PD é independente do espaço de endereçamento e
pode conter múltiplas threads, mas cada thread só pode pertencer a um único PD.
O projeto do MMP é guiado por três requisitos:

\begin{enumerate}
	\item \textbf{Diferente}: Cada PD diferente pode ter um domínio distinto
				para a mesma região de memória;
	\item \textbf{Pequeno}: A granularidade de compartilhamento deve ser menor
				do que uma página;
	\item \textbf{Revogável}: Um PD é dono da sua própria região de memória e
				pode mudar a forma como outros domínios veem a sua memória.
\end{enumerate}

% TODO: FIGURA K
Os autores afirmam que MMP tem a \hltodo[semântica da segmentação]{Que tal uma ref para a seção em que você explica isso? Além disso, não lembro se tem uma discussão sobre as desvantagens da segmentação} sem os problemas
associados a ela. MMP fornece controle fino sobre a proteção dos dados
compartilhados, utiliza endereçamento linear, é compatível com a \hltodo[\emph{ISA}]{Dafuq?}
existentes, não necessita de registradores de segmentos, possui um mecanismo
simples de revogação de permissões e não tem ponteiros marcados. \hltodo[A Figura K]{Oops! Null Pointer Exception...}
mostra o comparativo entre a abordagem utilizando segmentos e MMP. Para que
MMP possa garantir o controle de acesso à memória, ele precisa verificar as
permissões de acesso feitas por cada operação de \emph{load/store}; isto
claramente consome mais recursos computacionais. Por isso, MMP é uma solução
que também depende da utilização de um hardware personalizado.

% TODO: FIGURA G

\hltodo[A Figura G]{Segmentation Fault} ilustra a arquitetura de hardware proposta para a implementação do
MMP; note que ela introduz muitos elementos novos. %TODO: Descrever o hardware

Da perspectiva do software, Mondrix \citep{mondrix} sugere a utilização de um
novo elemento chamado de Supervisor de Memória, dividido em duas partes:
uma parte superior, responsável pelo controle do acesso; e parte inferior, responsável
por escrever nas tabelas de permissão. A parte superior é a mais importante,
sendo responsável por manter e controlar as políticas, pela interface com o Kernel,
por rastrear objetos compartilhados e por implementar grupos de permissão. As
definições de permissão da memória são descritas por três elementos:

\begin{enumerate}
	\item \emph{Permissão de Acesso}: As permissões de acesso são definidas pelo
				seguinte conjunto de elementos: permissão do domínio, \emph{gates},
        tabela de \emph{stack};
	\item \emph{Dono da Memória}: Define quem tem autoridade sobre a região de
				memória (domínio). Note que o espaço de endereçamento é contínuo e sem
				sobreposição, logo, cada região tem apenas um dono;
	\item \emph{Permissões Exportadas}: É possível exportar as permissões de
				acesso, permitindo que outras regiões chamem códigos
				fora do seu domínio. Contudo, a exportação pode ser controlada, de forma
				que outros trechos que acessem o código tenham uma visão limitada da
				região.
\end{enumerate}

Dados esses elementos, o Supervisor de Memória atua tomando as decisões
referentes ao acesso a uma dada região da memória.

Um aspecto interessante de se notar do Supervisor de Memória é a sua separação
lógica do alocador de memória. Mondrix permite que o Kernel escolha qual
alocador de memória deseja utilizar; isso é possível porque um domínio solicita
memória para o alocador e o Supervisor só estabelece as permissões para a
memória solicitada.
% Gosto muito dessa ideia de separar o alocador do controlador de acesso à
% memória, na minha opinião em termos práticas essa é uma das ideias mais
% valiosas desse trabalho.

O Supervisor também é responsável por revogar permissões quando uma região de
memória é liberada, além de manter o rastreamento de qual domínio tem acesso a
qual região de memória. O Supervisor ainda gerencia o acesso à \emph{stack}
das \emph{threads} (uma \emph{thread} só pode controlar \emph{stacks} que estão
no seu domínio), cria e destrói proteções de domínio e também valida as
políticas de acesso.

\subsection{SpaceJMP}
\label{sec:mvas}

Toda vez que um processo é criado, ele inicializa diferentes estruturas de
dados relacionadas à sua execução. Um dos elementos fundamentais dos processos
é o espaço de endereçamento virtual (\emph{Virtual Address Space -- VAS}), que é
completamente acoplado aos processos. Normalmente, os VAS são maiores que a
memória física e são totalmente isolados de outros processos. Se um processo
precisar compartilhar dados com outros processos, será necessário criar e
gerenciar uma região de memória compatilhada que permita a escrita e/ou leitura
por outros processos. O atual modelo de VAS não dá suporte para
o compartilhamento de estruturas de dados entre processos com base em ponteiros;
normalmente, é necessário serializar os dados, o que tem se tornado um incômodo.

\begin{figure}[!h]
  \centering
  \includegraphics[width=.7\textwidth]{vas_vs_physical_address} 
  \caption{VAS e Memóra Física}
  \label{fig:vas_vs_physical} 
\end{figure}

No início dos anos 1990s, os processadores tinham poucos bits para o VAS, mas que eram suficientes
para endereçar toda a memória física disponível na época. Apesar disso, durante
esse período, o tamanho da memória cresceu consideravelmente e os processadores
não foram mais capazes de endereçar diretamente todo o endereço de memória disponível
\cite{crowley}. A Figura \ref{fig:vas_vs_physical} exemplifica o problema
mencionados acima: a parte da esquerda ilustra uma CPU com bits suficientes
para o VAS, que consegue endereçar todo o espaço de endereçamento
físico da memória. O lado direito da figura representa uma situação na qual o
VAS é menor do que a memória física. Assim, um um único processo não é capaz de
acessar toda a memória física disponível, impondo um limite ao processo
quanto ao tamanho que ele pode ter. Essa situação
não é uma novidade: enquanto a indústria não produzia
novos processadores com bits extras para elevar o tamanho da VAS, os
desenvolvedores tiveram que utilizar vários artifícios para superar o
problema de bits insuficientes. Hoje em dia, esse problema está sob controle,
uma vez que os processadores atuais possuem um grande espaço de endereçamento.
Contudo, estamos no início de uma geração\todo{``geração de computadores''? ``geração da computação''?} conhecida por \emph{data-centric},
que será dominada por grandes memórias não-voláteis \citep{outlook}. Assim,
simplesmente incrementar o total de bits para o VAS nas CPUs \hltodo[não sanará mais]{Na verdade, poderia sanar, mas tem os problemas que você menciona. Eu mudaria para ``não sanará tão facilmente esse problema, pois elevar o número de bits ...}
esse problema. Elevar o número de bits na VAS tem impactos na produção,
desempenho e consumo de energia \citep{spacejmp}.

Um dos problemas do modelo de processos atual é a representação
das estruturas de dados baseadas em ponteiros fora dos
limites dos processos. A serialização de dados ou ponteiros especiais podem ser
empregados para resolver este problema, contudo, ambos os métodos são
incômodos, ineficientes e, em muitos casos, confusos \citep{spacejmp}. O segundo
problema está associado com a tarefa de coordenar o compartilhamento de memória
entre vários processos, uma vez que esta é uma tarefa complicada e tediosa.

Para tentar resolver os problemas citados, \citet{spacejmp} propuseram uma
técnica chamada \emph{SpaceJMP}. Os autores propõem desacoplar o VAS dos
processos e permitir que um único processo tenha Múltiplos VAS (MVAS)
associados a si. Nesse contexto, o VAS é promovido para um objeto de
primeira-classe no SO e dá ao processo a habilidade de gerir os muitos
VAS.

\begin{figure}[!h]
  \centering
  \includegraphics[width=.7\textwidth]{solve_huge_address_memory}
  \caption{Resolvendo o problema de endereçar memórias físicas grandes}
  \label{fig:large_memory}
\end{figure}

A Figura \ref{fig:large_memory} ilustra como MVAS resolve o problema de
endereçar memórias físicas maiores do que o VAS. Basicamente, todo processo
pode criar multiplos VAS e alternar entre eles. Se os processos precisarem de mais
memória do que o disponível, basta criar um novo address space com o começo
apontando para uma região de memória não utilizada e, em seguida, anexar o novo
VAS ao processo. \hltodo[Com isso, torna-se possível que o processo faça trocas entre
as diferentes VAS]{Eu tiraria, já está dito antes}. O problema da serialização e compartilhamento de dados também pode
ser simplificado pelo MVAS: basta criar um VAS e permitir que
múltiplos processos se anexem a ele.

\begin{figure}[!h]
  \centering
  \includegraphics[width=.7\textwidth]{traditional_vs_mvas} 
	\caption[VAS e MVAS]{VAS e MVAS \citep{spacejmp}}
  \label{fig:traditional_vs_mvas} 
\end{figure}

A Figura \ref{fig:traditional_vs_mvas} ilustra a principal diferença entre um
processo com um VAS acoplado para um modelo no qual o VAS é desacoplado
(MVAS). O lado esquerdo da Figura mostra um processo tradicional, como explicado
na Seção \ref{sec:processos-e-threads}. O lado direito da figura representa um
processo com múltiplos VAS e diferentes informações dentro de cada \emph{Heap}
e segmentos de memória. Nesse novo cenário, a pilha, os dados e os segmentos
de texto são compartilhados entre os vários MVAS. Por fim, é importante
notar que os processos precisam manter a informação sobre o VAS atualmente em
execução.
 
Os autores da técnica fizeram uma implementação para GNU/Linux. A principal
funcionalidade do MVAS foi exposta para o usuário via \emph{system calls}. A
lista a seguir mostra o conjunto de funções expostas para o usuário:

\begin{itemize}
  \item \texttt{vas\_create()}: Cria um novo VAS com o nome indicado;
  \item \texttt{vas\_delete()}: Remove um VAS criado previamente;
  \item \texttt{vas\_find()}: Encontra um VAS de acordo com o nome;
  \item \texttt{vas\_attach()}: Anexa um VAS ao processo;
  \item \texttt{vas\_detach()}: Desanexa um VAS do processo;
  \item \texttt{vas\_switch()}: Muda o VAS atualmente instalado no processo para outro.
\end{itemize}

\begin{figure}[!h]
  \centering
  \includegraphics[width=.7\textwidth]{mvas_example} 
	\caption[System Call disponibilizada pelo SpaceJMP]{System Call disponibilizada pelo SpaceJMP \cite{ellarge}}
  \label{fig:mvas_example}
\end{figure}

A Figura \ref{fig:mvas_example} ilustra o comportamento das chamadas de sistema
implementadas. O topo da figura mostra um processo normal com um VAS padrão
associado a si; em seguida, a chamada para a função \texttt{vas\_create()} é
feita de forma a criar um novo VAS. Após a criação, é preciso anexar o VAS ao
processo por meio da chamada de sistema \texttt{vas\_attach()}. Por fim, o
processo pode mudar entre os VAS por meio da chamada para
\texttt{vas\_switch()}.

\subsection{Light-weight Contexts}
\label{sec:lwc}

O fluxo de execução do processo é controlado pelo \textit{Program Counter (PC)}
juntamente com os dados da memória. Por sua vez, o escalonador é a entidade que
orquestra quando retirar e inserir um novo processo para execução na CPU. Consequentemente,
tanto o PC quando os demais dados do processo são mantidos juntos no nível do
kernel.

\citet{litton} afirmam que o desacoplamento do isolamento da memória, estado de
execução e separação dos privilégios dos processos trazem benefícios. Com tal
ideia em mente, os atores sugerem uma nova abstração de processos chamada de
\textit{lightweight-context (lwC)}. Basicamente, cada processo pode ter um
(lwC root) ou mais lwCs, cada um com o seu próprio mapeamento
virtual da memória, descritor de arquivos e credenciais. Opcionalmente, a API
do lwC permite controlar quais elementos podem ser compartilháveis ou não entre
os lwCs.

Um lwC compreende um mapeamento de memória virtual, uma coleção de mapeamento
de páginas, \emph{bindings} de descritores de arquivos e conjuntos de
credenciais; sempre que um novo processo é criado, o sistema cria um novo lwC
para ele. A aplicação pode acessar todas as funcionalidades do lwC via espaço
do usuário através de chamadas de sistemas que fazem o controle fino dos
comportamentos fornecidos. As chamadas mais interessantes são
\texttt{lwCreate()} e \texttt{lwSwitch()}. \texttt{lwCreate()} tem uma
semântica similar a \texttt{fork()}: \hltodo[depois que \texttt{lwCreate()} retorna,
o atual processo tem um novo lwC (filho) associado consigo mesmo que consiste
em um \emph{snapshot} do processo que foi chamado]{Cumé???? Não entendi nada}. Esse \emph{snapshot}
difere de \texttt{fork()} uma vez que nenhum PID novo ou thread é criado.
Depois da criação do processo filho, a aplicação é livre para fazer a troca
(com o \texttt{lwSwitch()}) para algum de seus \emph{snapshots} a qualquer
momento.

A Figura \ref{fig:lwc} ilustra a operação de troca entre diferentes lwCs. Um
processo pode criar vários lwCs durante a sua execução e em
diferentes momentos e pode
seletivamente escolher transferir o seu fluxo de execução para outro lwC por
meio de uma chamada para \texttt{lwSwitch()}. A mudança da thread é feita pelo
SO, que é responsável por mudar automaticamente o mapeamento da memória virtual,
as entradas das tabelas de arquivos, o \hltodo[stack de instrução]{? Não é o PC?} e o \emph{stack
pointer}.

\begin{figure}[!h]
  \centering
  \includegraphics[width=\textwidth]{lwC} 
  \caption{Exemplo do comportamento do lwC}
  \label{fig:lwc} 
\end{figure}

A interface de lwC também permite que outras operações, como compartilhamento
dinâmico ou estático de recursos, controle sobre o acesso as
\emph{capabilities}, emulação de chamadas de sistema e novas camadas de
isolamento dentro do processo. Todo o conjunto de chamadas de sistema do lwC
fornece um sofisticado modelo de programação que permite novas capacidades
tais quais rápido \emph{roll-back}, isolamento de seções e compartimentos de
proteção. A API do lwC é composta das seguintes funções:

\todo[inline]{Só jogar assim é sacanagem... tem que explicar um pouquinho cada uma e talvez até tirar as que não forem muito importantes}

\begin{itemize}
  \item \texttt{lwCreate(resource-spec,options)}
  \item \texttt{lwSwitch(target, args)}
  \item \texttt{lwRestrict(l, resource-spec)}
  \item \texttt{lwOverlay(l, resource-spec)}
  \item \texttt{lwSyscall(target, mask, syscall, syscall-args)}
\end{itemize}

Os autores demonstram os conceitos do lwC em uma implementação no FreeBSD. As
modificações principais foram feitas \hltodo[na memória]{Ééééé... no gerenciamento de memória, talvez?} e estruturas de dados do
processo. Seu uso foi demonstrado em adaptações feitas no Apache, Nginx e
OpenSSH. Por fim, os autores propõem certos algoritmos que discutiremos no
Capítulo \ref{cap:analise-sobre-abstracoes-de-processos}.

\section{Implementação Independente}

As \textit{Implementações Independentes} são aquelas que se propõem a criar um
SO totalmente novo, visando demonstrar um ou mais conceitos. Essas abordagens têm
poucas chances de serem adotadas diretamente, contudo apresentam conceitos e ideias que
podem ser interessantes para serem estendidas para SOs atuais. Pode-se dizer
que é relativamente comum que um SO copie características interessantes de
outros, por isso apresentamos \hltodo[alguns]{``Alguns'', na real é só um, o exokernel} SO usados apenas em contextos de pesquisa
mas que podem inspirar os SO de produção em uso hoje.

\subsection{Exokernel}

\citet{exokernel} defendem que os projetos de SOs possuem uma série de
desvantagens por terem várias abstrações diretamente programadas no núcleo. Os
autores argumentam que as abstrações implementadas diretamente no SO possuem
sérios impactos no desempenho, reduz consideravelmente a flexibilidade das
aplicações e limita as possibilidades de ampliar as funcionalidade
disponibilizadas no espaço do usuário. A implementação dessas abstrações
diretamente no núcleo do SO são problemática uma vez que: impedem a aplicação
de tirar proveito das otimizações que são especificas do seu domínio, as
mudanças em tais abstrações são desencorajadas e reduzem as possibilidade do
que pode ser feito no espaço de usuário.

Segundo os autores, a fixação das abstrações de alto-nível apresentam as
seguintes limitações:

\begin{itemize}
  \item A degradação do desempenho ocorre uma vez que atender ao maior número
        de aplicações possível com a mesma abstração tem um custo
        computacional. Por exemplo, algumas aplicações podem sofrer com o
        \emph{overhead} gerado por outras camadas de abstrações na qual não
        precisa;
  \item As informações de utilização de recursos são escondidas das aplicações,
        isto dificulta a aplicação ter controle sobre os recursos utilizados;
  \item As aplicações ficam limitadas a utilizar apenas aquelas interfaces
        padrão fornecidas pelo SO.
\end{itemize}

Por fim, com esses problemas em mente e motivados pela antiga observação de que:
quanto mais baixo o nível das primitivas (simples), mais eficientemente ela
pode ser implementada e maior a amplitude de ação fornecida para abstrações de
alto-nível; os autores propuseram uma nova arquitetura de SO chamada
\emph{exokernel}.

\emph{Exokernel} busca ser um núcleo mínimo que permita multiplexar de forma
segura os recursos de hardware e ao mesmo tempo fornecer uma interface de
baixo-nível na qual as abstrações do SO podem ser construída sobre ela. Para
que seja possível implementar um SO usando este conceito é preciso implementar
a chamada \textit{biblioteca do SO} com acesso aos recursos de baixo nível e
com as abstrações construídas nela (e.g. processos, arquivos, escalonador,
etc). Para que a implementação de tal conceito seja possível, os autores os
seguintes princípios:

\begin{enumerate}
  \item \textbf{\emph{Expor o hardware de forma segura:}} O conceito
        fundamental do \emph{exokernel} consistem em exportar de forma segura
        e controlada o acesso de baixo nível ao hardware, por isto toda
        implementação do \emph{exokernel} deve concentrar-se em exportar os
        privilégios e recursos de máquina. Por isto ele não deve impor
        abstrações de alto-nível, i.e, o \emph{exokernel} deve evitar gerir
        recursos;
  \item \textbf{\emph{Expor alocações:}} Um \emph{exokernel} deve permitir que
        uma biblioteca de SO aloque recursos. Além disto, os recursos não devem
        ser implicitamente alocados uma vez que a biblioteca de SO tem a
        obrigação de participar de todas as decisões de alocação;
  \item \textbf{\emph{Expor nomes:}} Um \emph{exokernel} deve exportar nomes
        físicos. Isto é eficiente uma vez que reduz o nível de indireção, e por
        sua vez o número de traduções necessárias entre nomes virtuais e
        físicos. Por fim expor nomes físicos também encapsula atributos de
        recursos uteis, por exemplo, um sistema de caches fisicamente indexado
        e diretamente mapeado, tem o nome da página física (i.e. o número da
        página) determina qual página que conflita;
  \item \textbf{\emph{Expor revogações:}} Um \emph{exokernel} deve utilizar um
        protocolo de revogação de recursos explicitas para que a biblioteca do
        SO possa gerir os recursos de forma eficiente;
\end{enumerate}

Por fim, além desses princípios o \emph{exokernel} também deve especificar as
políticas para arbitrar a competição entre as bibliotecas do SO. Dado esses
princípios o \emph{exokernel} deve disponibilizar as seguinte tarefas: ligação
segura, visibilidade de recursos e protocolo de falha.

A ligação segura (\emph{Secure Bindings - SB}) é o mecanismo que desacopla a
autorização do uso dos recursos. Para implementar o SB é preciso um conjunto de
primitivas que a aplicação pode usar para expressar verificações de proteção.
Essas podem ser implementadas em hardware ou software, por exemplo, a entrada
da TLB é uma primitiva de hardware. Na prática o SB trabalha com o conceito de
"baixar o código para o kernel", esse código é chamado para cada acesso ou
evento do recurso para determinar o dono e a ação que o Kernel deve tomar.
Baixar o código dentro do kernel permite uma thread da aplicação ter controle
sobre os eventos. Isto melhora o desempenho uma vez que elimina a camada
intermediaria do Kernel e também propícia limitar a aplicação aos seus
recursos.

A tarefa de \emph{revogação visível} é o mecanismo para recuperar os recursos
e romper com o SB estabelecidas. Vale observar que este mecanismo pode ser
visível ou invisível para a aplicação. Tradicionalmente os SOs realizam a
revogação de forma invisível desalocando recursos sem o envolvimento da
aplicação. O \emph{exokernel} utiliza revogação visível para a maioria dos
recursos, mesmo o processador é explicitamente revogado ao fim de um tempo de
execução determinado de forma que a biblioteca do SO é informada para que possa
reagir. O processo de revogação é como um diálogo entre o \emph{exokernel} e a
biblioteca do SO; a biblioteca precisa organizar uma lista de recursos que pode
ser desalocado rapidamente.

Por fim, um \emph{exokernel} pode definir um segundo estágio de revogação no
seu protocolo, na qual um pedido se torna um imperativo. Isto ajuda no caso em
que a biblioteca do SO falha, então o \emph{exokernel} pode simplesmente
quebrar o SB e informar a biblioteca. Todos as perdas de recursos forçadas são
armazenadas em um vetor de recuperação.

%TODO: Falta dar o fechamento com a parte dos processos e talz

% \subsection{Singularity}
% 
% \subsection{Corey}
